import os
import numpy as np
import librosa
import torch
from collections import Counter

# å‚æ•°è®¾å®š
DATASET_DIR = "C:/Users/hyc49/Desktop/p3"  
OUTPUT_FILE = "audio_features.pth"  # ä½¿ç”¨ .pth æ‰©å±•åä¾¿äº torch.save
SAMPLE_RATE = 22050
N_MELS = 128
N_FFT = 2048
HOP_LENGTH = 512
N_MFCC = 40  
FIXED_TIME_STEPS = 64  

def extract_features(audio_path):
    """è¯»å– WAV æ–‡ä»¶å¹¶æå–å¢å¼ºçš„éŸ³é¢‘ç‰¹å¾"""
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)

    # æå– Mel é¢‘è°±å›¾
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)
    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  

    # æå– MFCC åŠå…¶ä¸€é˜¶ã€äºŒé˜¶å·®åˆ†
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)
    mfcc_delta = librosa.feature.delta(mfcc)  
    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)  

    # æ–°å¢ç‰¹å¾ï¼šè‰²åº¦å›¾ã€é¢‘è°±å¯¹æ¯”åº¦ã€è°ƒå¼ç½‘ç»œ
    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)
    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)
    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)

    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    feature_matrix = np.vstack([mel_spec, mfcc, mfcc_delta, mfcc_delta2, chroma, spectral_contrast, tonnetz])  

    # ç¡®ä¿æ—¶é—´æ­¥é•¿ä¸€è‡´
    if feature_matrix.shape[1] > FIXED_TIME_STEPS:
        feature_matrix = feature_matrix[:, :FIXED_TIME_STEPS]  
    else:
        pad_width = FIXED_TIME_STEPS - feature_matrix.shape[1]
        feature_matrix = np.pad(feature_matrix, ((0, 0), (0, pad_width)), mode='constant')  

    # æ ‡å‡†åŒ–å½’ä¸€åŒ–
    feature_matrix = (feature_matrix - np.mean(feature_matrix)) / np.std(feature_matrix)

    return feature_matrix

def process_dataset(dataset_type):
    """å¤„ç† train/val æ•°æ®é›†"""
    audio_dir = os.path.join(DATASET_DIR, dataset_type)
    label_file = os.path.join(audio_dir, "labels.txt")

    # è¯»å–æ ‡ç­¾
    with open(label_file, "r") as f:
        labels = [int(line.strip()) for line in f]

    # è·å–æ‰€æœ‰ .wav æ–‡ä»¶ï¼Œç¡®ä¿æ’åºä¸€è‡´
    filenames = sorted([f for f in os.listdir(audio_dir) if f.endswith(".wav")])

    features = []
    num_files = len(filenames)
    processed_files = 0

    print(f"\nğŸ“Œ å¤„ç† {dataset_type} æ•°æ®é›†ï¼Œå…± {num_files} ä¸ªæ ·æœ¬...")

    for i, file in enumerate(filenames):
        audio_path = os.path.join(audio_dir, file)
        if os.path.exists(audio_path):
            feature_matrix = extract_features(audio_path)
            features.append({
                "filename": file,
                "feature_matrix": feature_matrix,
                "label": labels[i]
            })
            processed_files += 1
        else:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° {audio_path}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ã€‚")

        if (i + 1) % 10 == 0 or (i + 1) == num_files:
            print(f"  âœ… è¿›åº¦: {i + 1}/{num_files} ({(i + 1) / num_files * 100:.1f}%)")

    print(f"ğŸ¯ {dataset_type} æ•°æ®é›†å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_files} ä¸ªæ ·æœ¬ã€‚\n")
    return features, labels

if __name__ == "__main__":
    # å¤„ç† train å’Œ val æ•°æ®é›†
    train_features, train_labels = process_dataset("train")
    val_features, val_labels = process_dataset("val")

    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    label_counts = Counter(train_labels + val_labels)
    print("ğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š", label_counts)

    # è½¬æ¢ä¸º PyTorch Tensor æ ¼å¼
    train_features = [{"feature_matrix": torch.tensor(item["feature_matrix"], dtype=torch.float32), "label": item["label"]}
                      for item in train_features]
    val_features = [{"feature_matrix": torch.tensor(item["feature_matrix"], dtype=torch.float32), "label": item["label"]}
                    for item in val_features]

    # ä¿å­˜æ•°æ®
    torch.save({"train": train_features, "val": val_features}, OUTPUT_FILE)
    print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå·²ä¿å­˜è‡³ {OUTPUT_FILE}\n")
